<h1>Tensorflow Basics</h1>

<h2>HOW NEURAL NETWORKS FUNCTION</h2>
Artificial Neural Networks (ANN) can be best understood with the analogy of a stringed guitar.
Just as a collection of strings on a guitar may be tuned to create a specific chord when strummed together. Similarly, 
the artificial neural network is "tuned" to a given set of training data. As a guitar string is tightened or loosened, 
it becomes more or less in tune with a specific note. The weight of the adjustment of one string 
results in the other strings requiring adjustment. As we move through each string of the guitar, we will gradualy 
reduce errors in pitch to finally arrive as a resonably tuned guitar able to create the chord when 
we desire when we strum our guitar.

We may also imagine the weight of each string connecting a series of tuning pegs (neurons) and an iterative 
process to achieve proper tuning (training data). In each iteration there is more fine tuning (back-propagation)
to adjust  to the desired pitch. Eventually, the instrument is tuned when played (used for prediction) it will harmonize properly
(have acceptably low error rates).

https://cdn-images-1.medium.com/max/800/1*CcQPggEbLgej32mVF2lalg.png

  Activity: Building a "Toy" ANN






<h2>WHAT IS TENSORFLOW?</h2>
   Tensorflow is an open source platform specialized in performing machine learning calculations. 
   Developed by Google, it provides developers comprehensive tools, libraries, and community resources that 
   allow the world to access state-of-the-art machine learning resources to easily build and 
   launch applications.

   Google describes their framework as, "A software library for numerical computation using data flow graphs. 
   Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional 
   data arrays (tensors) communicated between them."

   <h3>What is a tensor?</h3>
   Tensors appear everywhere in machine learning equations. Some examples of tensors may include a<strong>vector</strong>, 
   which is a list of values or a <strong>matrix</strong>, which is a table. We can combine these to make lists of tables, tables of tables, etc.

   In essense, the execution of the mathematical series creates the <strong>flow</strong> across tensors, which makes this framework so powerful.

     <h4>Multi-layer neural network example</h4>

     In this example we see input data features (‘x1’, ‘x2’, …) going through 2 hidden layers, each with nodes (‘neurons’), 
     each with weights (‘W’) and bias (‘b’), the output is ('y').

     https://cdn-images-1.medium.com/max/800/1*mf-zDRbORrug-ki20TJKzg.png

     This becomes a series (a ‘flow’) of numerical computation (‘math’) going from input (‘X’) to output (‘y’). 
     The math involves multi-dimensional matrices (‘tensors’), constants and variables (eg. bias ‘b1’).
     
     https://cdn-images-1.medium.com/max/800/1*YmtjaKz0Pd8Z2gTYAdpk4A.png
   
     As a result of repeating this ‘flow’ the weights and biases adjust to ‘fit’ the expected output and a model is built.
     The framework can have some of the math performed on faster ‘GPU’ processors, which is very useful when working with 
     large quantities of data. But besides this, a framework for doing a lot of math across a series of equations is useful 
     to organize and simplify the code. That’s what any framework strives to do.

     <h3>Toy Data</h3> 
     Let’s imagine an array of 5 bits, this is our input. The output is [0,1] or [1,0] depending on: the 1st and last bit 
     both being ON or not.

        [0, 1, 1, 1, 1], [0,1]
        [1, 1, 1, 1, 0], [0,1]
        [1, 1, 1, 0, 0], [0,1]
        [1, 1, 0, 0, 0], [0,1]
        [1, 0, 0, 0, 1], [1,0]
        [1, 1, 0, 0, 1], [1,0]
        [1, 1, 1, 0, 1], [1,0]
        [1, 0, 0, 1, 1], [1,0]

        The entire dataset and its output makes intuitive sense, you (or any reasonably sentient person) could find the pattern 
        instinctively just by looking at the data.

        What if we trained a machine-learning model on several of these patterns, then asked it to predict a 5-bit pattern it 
        hadn’t been trained on? Would it predict the correct result? That’s a good ‘toy’ problem to work through, and it is 
        ‘machine learning’, by definition: <strong>the software learns the patterns.</strong>

         Code: https://github.com/ugik/notebooks/blob/master/Tensorflow%20ANN.ipynb

         Python Notebook: https://ipython.org/ipython-doc/3/notebook/

         import numpy as np
         import random
         import tensorflow as tf





   https://chatbotslife.com/how-neural-networks-work-ff4c7ad371f7

https://www.tensorflow.org/

https://chatbotslife.com/deep-learning-in-7-lines-of-code-7879a8ef8cfb

https://chatbotslife.com/tensorflow-demystified-80987184faf7







Backpropagation =in its simplest form, measures given statistics this to make a model. 


Sources:

Technologies
NumPy
http://www.numpy.org/#numpy

sigmoid function =
maps any value to a value between 0 and 1. We use it to convert numbers to probabilities. It also has several other desirable properties for training neural networks.
One of the desirable properties of a sigmoid function is that its output can be used to create its derivative. If the sigmoid's output is a variable "out", then the 
derivative is simply out * (1-out), which is very efficient. Allowss us to normalize results to within small, managable numbers, in this case between 0 and 1.

gradient AKA sigmoid derivative = tells us about the slope of the curve on any point, this helps 
us understand how "far off" (i.e. how large our error)
we are from being "in tune". This like a guitar string tuner, telling us how far we are from being "in tune"
relative to a specific note.

Derivative = slope of the function at a given point 
Different points can have different slopes on a curve.
Pseudo-random generation = E.g. rand( ) functionted values are not truly "random" because a mathematical formula is used to generate the values.
Since the same sequence is generated each time the seed remains the same, the rand( ) function generates a pseudo-random sequence.

"seed" = to prevent the same sequence from being generated each time, use
srand(x) to change the seed value.




https://en.wikipedia.org/wiki/Sigmoid_function


Later...
Derivatives tutorial
https://www.khanacademy.org/test-prep/fr-twelveth-grade-math/les-derivees/introduction-aux-derivees/v/calculus-derivatives-1
https://en.wikipedia.org/wiki/Sigmoid_function
